# -*- coding: utf-8 -*-
"""test_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aJTUJb_tzCnxL0Mwu0ebcjTUnUZ43gNf
"""

# Importing necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
import torch.nn.functional as F
from torchvision.datasets import GTSRB
import cv2
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, recall_score
import warnings
warnings.filterwarnings('ignore')

def custom_preprocessing(image):
    # Convert to grayscale
    image_gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)

    # Resize to (44, 42)
    resized_image = cv2.resize(image_gray, (44, 42))

    # Apply CLAHE
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    clahe_image = clahe.apply(resized_image)

    # Sharpen the image
    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
    sharpened = cv2.filter2D(clahe_image, -1, kernel)

    # Convert back to PIL Image for compatibility
    return transforms.functional.to_pil_image(sharpened)

# Define a custom transform
adv_transform = transforms.Compose([
    transforms.Lambda(lambda img: custom_preprocessing(img)),
    transforms.Lambda(lambda img: transforms.functional.to_tensor(img)),  # Convert to tensor
])

# function for evaluting the model on test dataset
def test_model(model, test_loader):
    """
    Evaluate the final model on the test dataset

    Parameters:
      model : Classifier Model used
      test_loader : Testing DataLoader

    Returns:
      None
    """

    # Storing model on GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()

    # Vars to store intermediate results
    test_correct = 0
    test_total = 0
    all_preds = []
    all_labels = []

    # Initializing the Cross Entropy Loss
    criterion = nn.CrossEntropyLoss()
    test_loss = 0.0

    # Testing phase
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            test_total += labels.size(0)
            test_correct += predicted.eq(labels).sum().item()

            # Store predictions and labels for metrics calculation
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate metrics
    test_accuracy = 100. * test_correct / test_total
    test_loss = test_loss / len(test_loader)
    precision = precision_score(all_labels, all_preds, average='weighted')
    recall = recall_score(all_labels, all_preds, average='weighted')
    f1 = f1_score(all_labels, all_preds, average='weighted')

    # Print results
    print("Test Set Results:")
    print(f"Test Loss: {test_loss:.3f}")
    print(f"Test Accuracy: {test_accuracy:.2f}%")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1-Score: {f1:.3f}")

    return

# Self attention module for GTSRB Classifier Class
class SelfAttention(nn.Module):
    def __init__(self, in_channels):
        super(SelfAttention, self).__init__()
        self.query = nn.Conv2d(in_channels, in_channels//8, 1)
        self.key = nn.Conv2d(in_channels, in_channels//8, 1)
        self.value = nn.Conv2d(in_channels, in_channels, 1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        batch_size, channels, height, width = x.size()

        # Creating projections
        query = self.query(x).view(batch_size, -1, height * width).permute(0, 2, 1)
        key = self.key(x).view(batch_size, -1, height * width)
        value = self.value(x).view(batch_size, -1, height * width)
        attention = torch.bmm(query, key)
        attention = F.softmax(attention, dim=-1)

        # Applying attention to value
        out = torch.bmm(value, attention.permute(0, 2, 1))
        out = out.view(batch_size, channels, height, width)

        # Adding the residual connections
        return self.gamma * out + x

class GTSRBModelWithAttention(nn.Module):
    def __init__(self):
        super(GTSRBModelWithAttention, self).__init__()

        # Building 3 deep convolutional layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)

        # Creating 2 Attention layers after 2nd and 3rd convolutional layer
        self.attention1 = SelfAttention(64)
        self.attention2 = SelfAttention(128)

        # Pooling layer
        self.pool = nn.MaxPool2d(2, 2)

        # Calculate the size of flattened features
        # After 3 pooling layers: 32x32 -> 16x16 -> 8x8 -> 4x4
        self.flatten_size = self._get_flatten_size((1, 42, 44))

        # Building fully connected layers
        self.fc1 = nn.Linear(self.flatten_size, 512)
        self.bn4 = nn.BatchNorm1d(512)
        self.fc2 = nn.Linear(512, 43)

        # Adding a dropout layer
        self.dropout = nn.Dropout(0.5)

    def _get_flatten_size(self, input_shape):
        """Helper function to compute the flattened size."""
        with torch.no_grad():
            x = torch.zeros(1, *input_shape)
            x = self.pool(F.relu(self.bn1(self.conv1(x))))
            x = self.pool(F.relu(self.bn2(self.conv2(x))))
            x = self.pool(F.relu(self.bn3(self.conv3(x))))
        return x.numel()

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.attention1(x)
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        x = self.attention2(x)
        x = x.view(-1, self.flatten_size)
        x = self.dropout(F.relu(self.bn4(self.fc1(x))))
        x = self.fc2(x)
        return x

def main():
  # Testing the model
  # Creating a test dataloader
  test_dataset = GTSRB(root='./data', split='test', download=True, transform=adv_transform)

  test_loader = DataLoader(
          test_dataset,
          batch_size=64,
          shuffle=False,
          num_workers=4,
          drop_last=True
      )

  # Checking if GPU is available or not
  device='cuda' if torch.cuda.is_available() else 'cpu'

  # Loading the best model weights
  model = GTSRBModelWithAttention()
  model.load_state_dict(torch.load('best_model.pth', map_location=torch.device(device)))

  # Test the model
  test_model(model, test_loader)

if __name__ == '__main__':
    main()