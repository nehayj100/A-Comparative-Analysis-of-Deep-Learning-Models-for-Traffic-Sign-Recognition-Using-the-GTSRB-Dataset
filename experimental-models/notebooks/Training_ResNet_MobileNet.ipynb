{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CNN with Adam Optimizer and a LR Scheduler**\n",
        "Image Transformations Used:\n",
        "\n",
        "Normalization\n",
        "Params:\n",
        "\n",
        "Epochs - 30\n",
        "Learning Rate - 0.001"
      ],
      "metadata": {
        "id": "iJfaAZWJVmKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3DvcOysYfLN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "\n",
        "from PIL import Image, ImageOps, ImageEnhance, ImageFilter\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.datasets import GTSRB\n",
        "from torchvision import models\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk1NhFWvVjMW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9Zso1M3bMnR",
        "outputId": "4d952f7a-5080-4448-d989-d59c107741b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oemXlPi4p7sy"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_pFCn3MotbD"
      },
      "outputs": [],
      "source": [
        "root_dir = '/root/GTSRB'\n",
        "\n",
        "os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "# Applying the custom transform in the dataset loader\n",
        "root_dir = '/root/GTSRB'\n",
        "os.makedirs(root_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXjCk7aDV5l6",
        "outputId": "adfe5fb5-c66b-4c40-d634-16935afa782d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB-Training_fixed.zip to /root/GTSRB/gtsrb/GTSRB-Training_fixed.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 187M/187M [00:07<00:00, 23.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/GTSRB/gtsrb/GTSRB-Training_fixed.zip to /root/GTSRB/gtsrb\n",
            "Downloading https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip to /root/GTSRB/gtsrb/GTSRB_Final_Test_Images.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 89.0M/89.0M [00:04<00:00, 18.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/GTSRB/gtsrb/GTSRB_Final_Test_Images.zip to /root/GTSRB/gtsrb\n",
            "Downloading https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip to /root/GTSRB/gtsrb/GTSRB_Final_Test_GT.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 99.6k/99.6k [00:00<00:00, 220kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /root/GTSRB/gtsrb/GTSRB_Final_Test_GT.zip to /root/GTSRB/gtsrb\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "gtsrb_train = GTSRB(root=root_dir, split='train', transform=transform, download=True)\n",
        "\n",
        "# Load the test dataset\n",
        "gtsrb_test = GTSRB(root=root_dir, split='test', transform=transform, download=True)\n",
        "\n",
        "# Calculate the size of each split (half of the test dataset)\n",
        "test_size = len(gtsrb_test)\n",
        "validation_size = test_size // 2\n",
        "test_size = test_size - validation_size  # The other half for testing\n",
        "\n",
        "# Split the test data into validation and test\n",
        "validation_dataset, test_dataset = random_split(gtsrb_test, [validation_size, test_size])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxxJwyOAYni5",
        "outputId": "1fcb13bb-bb7b-4881-b113-4d0c0fe70ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 26640\n",
            "Number of total test samples: 12630\n",
            "Number of valid samples: 6315\n",
            "Number of test samples: 6315\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of training samples: {len(gtsrb_train)}')\n",
        "print(f'Number of total test samples: {len(gtsrb_test)}')\n",
        "print(f'Number of valid samples: {len(validation_dataset)}')\n",
        "print(f'Number of test samples: {len(test_dataset)}')\n",
        "\n",
        "# Determine the number of classes by looking at unique labels in the dataset\n",
        "train_labels = [label for _, label in gtsrb_train]\n",
        "test_labels = [label for _, label in test_dataset]\n",
        "vlaidation_labels = [label for _, label in validation_dataset]\n",
        "\n",
        "num_classes = len(set(train_labels))\n",
        "print(f'Number of classes: {num_classes}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAvtWu-_Yo5R"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn1qblKNYqU3"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(gtsrb_train, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQTA4yYcYrtj"
      },
      "outputs": [],
      "source": [
        "# plot for RGB\n",
        "def plot_random_images(dataset, num_images=5):\n",
        "    indices = random.sample(range(len(dataset)), num_images)\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        image, label = dataset[idx]\n",
        "        if image.shape[0] == 3:\n",
        "            image = np.transpose(image, (1, 2, 0))\n",
        "        axes[i].imshow(image)\n",
        "        axes[i].set_title(f'Label: {label}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot 5 random images from the training dataset\n",
        "plot_random_images(gtsrb_train, num_images=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gul5K4lfTXt_"
      },
      "outputs": [],
      "source": [
        "# Load pretrained MobileNetV2 model (for RGB images, 3 channels)\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Modify the final classifier layer to match the number of classes in GTSRB\n",
        "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device).float()\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct / total\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_loader:\n",
        "            inputs = inputs.to(device).float()\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_losses.append(val_running_loss / len(validation_loader))\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n",
        "          f\"Val Loss: {val_running_loss/len(validation_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "          f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Save model weights\n",
        "# torch.save(model.state_dict(), \"/content/drive/MyDrive/mobilenetv2_gtsrb_Neha_Kim_Jonathan.pth\")\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Evaluation on test data\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device).float(), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJxuqdvhTaSB"
      },
      "outputs": [],
      "source": [
        "# Load pretrained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modify the final fully connected layer to match the number of classes\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device).float()\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct / total\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_loader:\n",
        "            inputs = inputs.to(device).float()\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_losses.append(val_running_loss / len(validation_loader))\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n",
        "          f\"Val Loss: {val_running_loss/len(validation_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "          f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Save model weights\n",
        "# torch.save(model.state_dict(), \"/content/drive/MyDrive/resnet_gtsrb_Neha_Kim_Jonathan.pth\")\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Evaluation on test data\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device).float(), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Compute precision, recall, and F1 score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z1B4ySZT94_"
      },
      "outputs": [],
      "source": [
        "# EOF"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}