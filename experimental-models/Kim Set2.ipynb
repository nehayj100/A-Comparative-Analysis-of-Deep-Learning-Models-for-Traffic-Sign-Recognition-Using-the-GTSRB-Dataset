{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"I_emcdZr5A2x"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/30]\n","Training Loss: 2.602\n","Training Accuracy: 27.47%\n","Training Precision: 0.239, Recall: 0.275, F1-Score: 0.239\n","Testing Accuracy: 42.95%\n","Testing Precision: 0.452, Recall: 0.430, F1-Score: 0.392\n","Best Testing Accuracy: 42.95%\n","--------------------\n","Epoch [2/30]\n","Training Loss: 2.013\n","Training Accuracy: 40.99%\n","Training Precision: 0.386, Recall: 0.410, F1-Score: 0.384\n","Testing Accuracy: 51.97%\n","Testing Precision: 0.540, Recall: 0.520, F1-Score: 0.504\n","Best Testing Accuracy: 51.97%\n","--------------------\n","Epoch [3/30]\n","Training Loss: 1.792\n","Training Accuracy: 46.81%\n","Training Precision: 0.461, Recall: 0.468, F1-Score: 0.450\n","Testing Accuracy: 58.00%\n","Testing Precision: 0.580, Recall: 0.580, F1-Score: 0.563\n","Best Testing Accuracy: 58.00%\n","--------------------\n","Epoch [4/30]\n","Training Loss: 1.656\n","Training Accuracy: 50.74%\n","Training Precision: 0.507, Recall: 0.507, F1-Score: 0.491\n","Testing Accuracy: 59.70%\n","Testing Precision: 0.634, Recall: 0.597, F1-Score: 0.586\n","Best Testing Accuracy: 59.70%\n","--------------------\n","Epoch [5/30]\n","Training Loss: 1.543\n","Training Accuracy: 53.44%\n","Training Precision: 0.544, Recall: 0.534, F1-Score: 0.522\n","Testing Accuracy: 62.14%\n","Testing Precision: 0.634, Recall: 0.621, F1-Score: 0.601\n","Best Testing Accuracy: 62.14%\n","--------------------\n","Epoch [6/30]\n","Training Loss: 1.461\n","Training Accuracy: 55.71%\n","Training Precision: 0.565, Recall: 0.557, F1-Score: 0.544\n","Testing Accuracy: 62.60%\n","Testing Precision: 0.666, Recall: 0.626, F1-Score: 0.618\n","Best Testing Accuracy: 62.60%\n","--------------------\n","Epoch [7/30]\n","Training Loss: 1.390\n","Training Accuracy: 57.51%\n","Training Precision: 0.595, Recall: 0.575, F1-Score: 0.566\n","Testing Accuracy: 64.55%\n","Testing Precision: 0.688, Recall: 0.646, F1-Score: 0.639\n","Best Testing Accuracy: 64.55%\n","--------------------\n"]}],"source":["import pandas as pd\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import GTSRB\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","# Image preprocessing function\n","def image_processing_function(image_list):\n","    augmented_image = []\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n","    df = pd.read_csv(\"Annotate.csv\", header=None)\n","    annotate = df.values.tolist()\n","\n","    for i in range(len(image_list)):\n","        image_gray = cv2.cvtColor(image_list[i], cv2.COLOR_BGR2GRAY)\n","        cropped_image = image_gray[\n","            int(annotate[i][1]):int(annotate[i][3]),\n","            int(annotate[i][0]):int(annotate[i][2])\n","        ]\n","        cropped_image = cv2.resize(cropped_image, (44, 42))\n","        clahe_image = clahe.apply(cropped_image)\n","        kernel = np.array([\n","            [0, -1, 0],\n","            [-1, 5, -1],\n","            [0, -1, 0]\n","        ])\n","        sharpened = cv2.filter2D(clahe_image, -1, kernel)\n","        augmented_image.append(sharpened)\n","    return augmented_image\n","\n","class CustomGTSRBDataset(Dataset):\n","    def __init__(self, dataset, annotations_file):\n","        self.dataset = dataset\n","        self.annotations_file = annotations_file\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, label = self.dataset[idx]  # Get image and label\n","        image = np.array(image)  # Convert PIL image to NumPy array\n","        processed_image = image_processing_function([image])[0]  # Apply preprocessing\n","        processed_image = torch.tensor(processed_image, dtype=torch.float32).unsqueeze(0)  # Convert to tensor\n","        return processed_image, label\n","\n","\n","# Load GTSRB dataset\n","root_dir = './data/GTSRB'\n","gtsrb_train = GTSRB(root=root_dir, split='train', download=True)\n","gtsrb_test = GTSRB(root=root_dir, split='test', download=True)\n","\n","# Apply custom dataset with preprocessing\n","train_dataset = CustomGTSRBDataset(gtsrb_train, \"Annotate.csv\")\n","test_dataset = CustomGTSRBDataset(gtsrb_test, \"Annotate.csv\")\n","\n","# Data loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","class TrafficSignCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(TrafficSignCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.dropout = nn.Dropout(0.5)\n","\n","        # Calculate the flattened feature size dynamically\n","        self._to_linear = None\n","        self._initialize_flattened_size((1, 42, 44))  # Input image size (channels, height, width)\n","\n","        self.fc1 = nn.Linear(self._to_linear, 256)\n","        self.fc2 = nn.Linear(256, num_classes)\n","\n","    def _initialize_flattened_size(self, input_size):\n","        dummy_input = torch.zeros(1, *input_size)\n","        x = self.pool(torch.relu(self.bn1(self.conv1(dummy_input))))\n","        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n","        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n","        self._to_linear = x.numel()  # Total number of elements in the flattened tensor\n","\n","    def forward(self, x):\n","        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n","        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n","        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n","        x = x.view(x.size(0), -1)  # Dynamically flatten the tensor\n","        x = torch.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","\n","# Define training setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Get the number of unique classes\n","num_classes = len(set(sample[1] for sample in gtsrb_train))\n","model = TrafficSignCNN(num_classes=num_classes).to(device)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training and validation loops\n","train_losses, train_accuracies = [], []\n","test_losses, test_accuracies = [], []\n","best_test_acc = 0.0\n","epochs = 30\n","\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss, correct_train, total_train = 0.0, 0, 0\n","    all_train_preds, all_train_labels = [], []\n","\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","\n","        if outputs.shape[0] != labels.shape[0]:\n","            raise ValueError(f\"Mismatch in batch sizes: outputs={outputs.shape[0]}, labels={labels.shape[0]}\")\n","\n","        loss = loss_function(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","        all_train_preds.extend(predicted.cpu().numpy())\n","        all_train_labels.extend(labels.cpu().numpy())\n","\n","    train_loss = running_loss / len(train_loader)\n","    train_accuracy = 100 * correct_train / total_train\n","    train_losses.append(train_loss)\n","    train_accuracies.append(train_accuracy)\n","\n","    train_precision = precision_score(all_train_labels, all_train_preds, average='weighted', zero_division=0)\n","    train_recall = recall_score(all_train_labels, all_train_preds, average='weighted', zero_division=0)\n","    train_f1 = f1_score(all_train_labels, all_train_preds, average='weighted', zero_division=0)\n","\n","    # Validation loop\n","    model.eval()\n","    test_running_loss, correct_test, total_test = 0.0, 0, 0\n","    all_test_preds, all_test_labels = [], []\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","\n","            if outputs.shape[0] != labels.shape[0]:\n","                raise ValueError(f\"Mismatch in batch sizes: outputs={outputs.shape[0]}, labels={labels.shape[0]}\")\n","\n","            loss = loss_function(outputs, labels)\n","            test_running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_test += labels.size(0)\n","            correct_test += (predicted == labels).sum().item()\n","\n","            all_test_preds.extend(predicted.cpu().numpy())\n","            all_test_labels.extend(labels.cpu().numpy())\n","\n","    test_loss = test_running_loss / len(test_loader)\n","    test_accuracy = 100 * correct_test / total_test\n","    test_losses.append(test_loss)\n","    test_accuracies.append(test_accuracy)\n","\n","    test_precision = precision_score(all_test_labels, all_test_preds, average='weighted', zero_division=0)\n","    test_recall = recall_score(all_test_labels, all_test_preds, average='weighted', zero_division=0)\n","    test_f1 = f1_score(all_test_labels, all_test_preds, average='weighted', zero_division=0)\n","\n","    if test_accuracy \u003e best_test_acc:\n","        best_test_acc = test_accuracy\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","    print(f'Epoch [{epoch+1}/{epochs}]')\n","    print(f'Training Loss: {train_loss:.3f}')\n","    print(f'Training Accuracy: {train_accuracy:.2f}%')\n","    print(f'Training Precision: {train_precision:.3f}, Recall: {train_recall:.3f}, F1-Score: {train_f1:.3f}')\n","    print(f'Testing Accuracy: {test_accuracy:.2f}%')\n","    print(f'Testing Precision: {test_precision:.3f}, Recall: {test_recall:.3f}, F1-Score: {test_f1:.3f}')\n","    print(f'Best Testing Accuracy: {best_test_acc:.2f}%')\n","    print('--------------------')\n","\n","# Plotting Metrics\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","ax1.plot(train_losses, label='Train Loss')\n","ax1.plot(test_losses, label='Test Loss')\n","ax1.legend()\n","ax1.set_title('Loss vs Epochs')\n","ax2.plot(train_accuracies, label='Train Accuracy')\n","ax2.plot(test_accuracies, label='Test Accuracy')\n","ax2.legend()\n","ax2.set_title('Accuracy vs Epochs')\n","plt.show()\n","\n","# Confusion Matrix\n","conf_matrix = confusion_matrix(all_test_labels, all_test_preds)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.show()\n"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyP9Hea+JikOLramsXlkl1Rt","gpuType":"V28","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}